{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY5rppFa1ag5"
      },
      "source": [
        "# Install Spark\n",
        "Installing PySpark library, which is a Python API for Apache Spark, allowing for big data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uff1zfPJ1W99",
        "outputId": "6548b6b2-e894-4047-c8d6-9add071e75a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORys5sbYXqey"
      },
      "source": [
        "Importing the SparkSession library form pyspark.sql module to create a SparkSession, which is the entry point to programming Spark with the Dataset and DataFrame API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2nX21Hn9BsO"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzax1XogZLue"
      },
      "source": [
        "Creating a Spark session\n",
        "Initializes a SparkSession with configurations such as the app name and master node details.\n",
        "Here, the app will be named \"ML Fraud Pred\" and runs locally using all available cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD9wxgVU9Fhr"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"ML Fraud Pred\")\\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vM8dnKtZiV0"
      },
      "source": [
        "# Data Reading\n",
        "Loading a CSV file into a Spark DataFrame with column headers and automatically infers column data types.\n",
        "The df.printSchema prints the schema of the DataFrame, which shows the column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDnHxMv29IcW",
        "outputId": "d7feca33-ab7e-471a-9dc4-40b4bbe39d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- step: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- nameOrig: string (nullable = true)\n",
            " |-- oldbalanceOrg: double (nullable = true)\n",
            " |-- newbalanceOrig: double (nullable = true)\n",
            " |-- nameDest: string (nullable = true)\n",
            " |-- oldbalanceDest: double (nullable = true)\n",
            " |-- newbalanceDest: double (nullable = true)\n",
            " |-- isFraud: integer (nullable = true)\n",
            " |-- isFlaggedFraud: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('/content/Fraud_Data1.csv', header = True, inferSchema = True)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg6QKWmzRNHT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vlxOfLZ3Ef"
      },
      "source": [
        "This command shows the first 5 rows of the DataFrame to get a quick overview of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chup5RxA9gx3",
        "outputId": "e9868ddd-3368-4781-836b-42bf058ede0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
            "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
            "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
            "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKWMqkSPaQ_U"
      },
      "source": [
        "# Filtering the Data and Performing Undersampling\n",
        "Creates two seperate DataFrames by filtering the original DataFrame based on the value of the 'isFraud' column.\n",
        "\n",
        "The counts the number of rows in each DataFrame, representing fraud and non-fraud instances.\n",
        "\n",
        "Calculates the ratio of fraud to non-fraud instances to use for understanding the majority class.\n",
        "\n",
        "Creates a new DataFrame from the non-fraud DataFrame by sampling a fraction of its instance without replacement, where the fraction is determined by the previously calculated ratio.\n",
        "\n",
        "Combining the fraud DataFrame with the undersampled non-fraud DataFrame to create a more balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPgHMFOSCgYi"
      },
      "outputs": [],
      "source": [
        "# Separate the two classes\n",
        "fraud_df = df.filter(df.isFraud == 1)\n",
        "non_fraud_df = df.filter(df.isFraud == 0)\n",
        "\n",
        "# Count the instances\n",
        "fraud_count = fraud_df.count()\n",
        "non_fraud_count = non_fraud_df.count()\n",
        "\n",
        "# Calculate the ratio to undersample the larger class\n",
        "ratio = fraud_count / non_fraud_count\n",
        "\n",
        "# Perform undersampling\n",
        "undersampled_non_fraud_df = non_fraud_df.sample(False, ratio)\n",
        "\n",
        "# Combine back the undersampled non-fraud data with the fraud data\n",
        "balanced_df = fraud_df.union(undersampled_non_fraud_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT4CpIPWbi9X"
      },
      "source": [
        "Importing necessary classes for feature transformation and machine learning model\n",
        "\n",
        "Preparing the data for machine learning\n",
        "Selects columns to be used as features and the target variable, then uses VectorAssembler to transform these columns into a single vector column.\n",
        "\n",
        "Applying the transformation\n",
        "Transforms the balanced DataFrame and selects only the features vector and target variable for the machine learning algorithms.\n",
        "\n",
        "Splitting the data into training and test sets\n",
        "Randomly splits the data into training and test sets with 70-30 split and a seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN-0er3vCpI3"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assuming `balanced_df` is your DataFrame\n",
        "\n",
        "# First, index the string column to numeric indices\n",
        "stringIndexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
        "\n",
        "# Then, one-hot encode these indices\n",
        "encoder = OneHotEncoder(inputCols=[\"typeIndex\"], outputCols=[\"typeVec\"])\n",
        "\n",
        "# Define other feature columns that don't need encoding\n",
        "featureCols = [\"step\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\"]\n",
        "\n",
        "# Assemble all feature columns (numeric + one-hot encoded) into a single vector\n",
        "assembler = VectorAssembler(inputCols=featureCols + [\"typeVec\"], outputCol=\"features\")\n",
        "\n",
        "# Define a pipeline that executes the steps in sequence\n",
        "pipeline = Pipeline(stages=[stringIndexer, encoder, assembler])\n",
        "\n",
        "# Transform the data\n",
        "transformed_data = pipeline.fit(balanced_df).transform(balanced_df)\n",
        "\n",
        "# Now, select only the features vector and the label for ML algorithms\n",
        "data = transformed_data.select(\"features\", \"isFraud\")\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx8oSCQJcb6H"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQt2nVJ1CrSJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"isFraud\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# Train the model\n",
        "lrModel = lr.fit(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qLkEdaXCu4l",
        "outputId": "651640b5-0134-4080-bff1-4b655c9c01de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.9591836734693877\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = lrModel.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test AUC: {auc}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}